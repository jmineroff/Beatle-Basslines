{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BassGeneration.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/jmineroff/Beatle-Basslines/blob/master/BassGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"OBXDJWvwLQvw","colab_type":"code","colab":{}},"source":["# Optional code cell for running in Google Colab\n","# Links to local git folder in Google Drive and installs modules\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install pypianoroll\n","!pip install AudioConverter\n","!apt install fluidsynth\n","!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n","!pip install midi2audio\n","\n","!pip install tables\n","\n","import os\n","try:\n","  os.chdir(\"drive/My Drive/Beatle-Basslines\") # Local git path\n","except Exception:\n","  pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eyykK17l1wod","colab_type":"code","colab":{}},"source":["# Initialization\n","\n","from numpy.random import seed\n","seed(1)\n","from tensorflow import set_random_seed\n","set_random_seed(2)\n","\n","from midi2audio import FluidSynth as fs\n","from IPython.display import display, Audio\n","import numpy as np\n","import pandas as pd\n","from pypianoroll import Multitrack, Track\n","from matplotlib import pyplot as plt\n","import os\n","import sys\n","from scipy import sparse\n","\n","import pickle\n","import tables\n","\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","plt.rcParams[\"figure.figsize\"] = (20,10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRXvxj0k06Rd","colab_type":"code","colab":{}},"source":["# Helper functions\n","\n","def write_soundfile(midifile, output): # Write midi to an audio file\n","  fs(sound_font=\"font.sf2\", sample_rate=11025).midi_to_audio(midifile, output)\n","\n","\n","def piano_plot(pianofile): # \n","  fig, axs = pianofile.plot()\n","  plt.show()\n","\n","\n","def parse_midi_dir(list_songs=True, list_tracks=False): # Parse directory of raw midi files\n","  num_midi = 0\n","\n","  for subdir, dirs, files in os.walk('RawMIDI/'):\n","    for file in files:\n","      num_midi += 1\n","      \n","      if not file.endswith(\".mid\"):\n","        continue\n","        \n","      if list_songs:\n","        print(subdir.split('/')[-1],'-',file)\n","      \n","      if list_tracks:\n","        filepath = subdir + os.sep + file\n","        temp = Multitrack(filepath)\n","        for track in temp.tracks: # \n","          print('...',track.name)\n","\n","  print('Track count:',num_midi)\n","\n","def tempo_to_progress(tempo): # Get song progress (0-1) from beat-uniform tempo array\n","  return np.cumsum(1/tempo) / np.sum(1/tempo)\n","\n","\n","def add_songs_to_df(df, partial_track_match=False, binarize=False): # Add all songs to dataframe\n","  note_limits = {'Drums': [127,0], 'Bass': [127,0], 'Vocals': [127,0], 'Rhythm': [127,0], 'Lead': [127,0]}\n","  note_counts = {'Drums': 0, 'Bass': 0, 'Vocals': 0, 'Rhythm': 0, 'Lead': 0}\n","      \n","  for subdir, dirs, files in os.walk('RawMIDI/'): # Walk through all files in directory\n","    for file in files:\n","      \n","      track_to_channel = {'Drums': [], 'Bass': [], 'Vocals': [], 'Rhythm': [], 'Lead': [], 'VocalsAll': []}\n","      \n","      if not file.endswith(\".mid\"): # Only process MIDI files\n","        continue\n","\n","      print('Processing', subdir.split('/')[-1], '-', file) # Status update\n","\n","      filepath = subdir + os.sep + file\n","      temp = Multitrack(filepath)\n","\n","      for track_idx, track in enumerate(temp.tracks): # Parse all tracks in midi file\n","        for key in track_to_channel: # Compare all keys to track name\n","          if ( track.name == key ) or ( partial_track_match and track.name.startswith(key) ): # Exact or partial track-key match (e.g. 'Vocals2', 'Vocals3', etc.)\n","            track_to_channel[key].append(track_idx)\n","\n","      if not track_to_channel['Vocals']: # Copy index from 'VocalsAll' to 'Vocals' if necessary\n","        track_to_channel['Vocals'] = track_to_channel['VocalsAll']\n","      \n","      del track_to_channel['VocalsAll']\n","\n","      for key in track_to_channel: # Set index of nonexistent tracks to -1\n","        if not track_to_channel[key]:\n","          track_to_channel[key].append(-1)\n","      \n","      if binarize:\n","        temp.binarize()\n","\n","      full_pianoroll = temp.get_stacked_pianoroll()\n","      full_pianoroll = np.append(full_pianoroll, np.zeros((*full_pianoroll.shape[0:2],1)), axis=2) # Add extra track of zeros for nonexistent tracks\n","\n","      tempo = temp.tempo\n","      downbeat = temp.downbeat\n","\n","      progress = tempo_to_progress(tempo)\n","\n","      pianorolls = {}\n","\n","      # Get instrument pianorolls from full pianoroll using dictionary indices\n","      for key in track_to_channel: # \n","        \n","        if binarize:\n","          pianorolls[key] = np.amax(full_pianoroll[:,:,track_to_channel[key]], axis=2)\n","        else:\n","          pianorolls[key] = np.sum(full_pianoroll[:,:,track_to_channel[key]], axis=2)\n","\n","        note_locations = np.flatnonzero(np.amax(pianorolls[key], axis=0))\n","        note_count = np.amax(np.count_nonzero(pianorolls[key], axis=1))\n","\n","        #print(len(note_locations))\n","        #print(note_locations)\n","        #print(note_count)\n","        \n","        if not note_locations.any():\n","          continue\n","\n","        high_note = np.amin(note_locations)\n","        low_note = np.amax(note_locations)\n","        if high_note < note_limits[key][0]:\n","          note_limits[key][0] = high_note\n","        if low_note > note_limits[key][1]:\n","          note_limits[key][1] = low_note\n","        if note_count > note_counts[key]:\n","          note_counts[key] = note_count\n","      \n","      # Add new entry to dataframe\n","      df.loc[len(df)] = [file.split('.')[0], subdir.split('/')[-1], pianorolls['Drums'], pianorolls['Bass'], pianorolls['Vocals'], pianorolls['Rhythm'], pianorolls['Lead'], tempo, downbeat, progress]\n","\n","  return note_limits, note_counts\n","\n","\n","def manual_song_test(df, song_name): # Generate original and recombined audio for manual comparison\n","  # Build track from original MIDI file\n","  filepath = 'RawMIDI/' + df.loc[df.Song == song_name].Album.values[0] + '/' + song_name + '.mid'\n","\n","  original = Multitrack(filepath)\n","  #piano_plot(original)\n","\n","  original.write('temp/original.mid')\n","  write_soundfile('temp/original.mid', 'temp/original.mp3')\n","  print('Original File')\n","  display(Audio('temp/original.mp3'))\n","  \n","  # Build track from dataframe\n","  drum_track = Track(df.loc[df.Song == song_name].Drums.values[0], is_drum=True, name='Drums')\n","  bass_track = Track(df.loc[df.Song == song_name].Bass.values[0], program=34, is_drum=False, name='Bass')\n","  vocal_track = Track(df.loc[df.Song == song_name].Vocals.values[0], program=73, is_drum=False, name='Vocals')\n","  rhythm_track = Track(df.loc[df.Song == song_name].Rhythm.values[0], program=24, is_drum=False, name='Rhythm')\n","  lead_track = Track(df.loc[df.Song == song_name].Lead.values[0], program=26, is_drum=False, name='Lead')\n","  tempo = df.loc[df.Song == song_name].Tempo.values[0]\n","  downbeat = df.loc[df.Song == song_name].Downbeat.values[0]\n","\n","  bass_track.transpose(-12) # Only intermittently needed for proper playback with FluidSynth Soundfont - don't use for exporting raw MIDI files\n","  \n","  recombined = Multitrack(tracks=[drum_track, bass_track, vocal_track, rhythm_track, lead_track], tempo=tempo, downbeat=downbeat)\n","\n","  recombined.write('temp/recombined.mid')\n","  write_soundfile('temp/recombined.mid', 'temp/recombined.mp3')\n","  print('Recombined File')\n","  display(Audio('temp/recombined.mp3'))\n","\n","\n","def raw_song_to_ndarray(df, song_idx, note_limits, note_counts, binarize_threshold=None): # Trim raw song data into ndarray\n","  inputs = ['Drums','Vocals','Rhythm','Lead','Tempo','Downbeat','Progress']  # 'Progress' is last input variable\n","  outputs = ['Bass']\n","  \n","  print(df.iloc[song_idx]['Song'])\n","\n","  # Initialize input array\n","  input_dim = 0\n","  for key in inputs:\n","    if key in note_limits:\n","      input_dim += note_limits[key][1] - note_limits[key][0] + 1\n","    else:\n","      input_dim += 1\n","\n","  # Initialize output array\n","  output_dim = 0\n","  for key in outputs:\n","    if key in note_limits:\n","      output_dim += note_limits[key][1] - note_limits[key][0] + 1\n","    else:\n","      output_dim += 1\n","\n","  for num, key in enumerate(inputs): # Build full inputs array\n","    if key in note_limits:\n","      new_input = df.iloc[song_idx][key][:,note_limits[key][0]:note_limits[key][1]+1][:,:]\n","\n","      if binarize_threshold is not None:\n","        new_input[new_input <= binarize_threshold] = 0\n","        new_input[new_input > binarize_threshold] = 1\n","    else:\n","      new_input = df.iloc[song_idx][key][:,np.newaxis]\n","\n","    if key == 'Tempo':\n","      new_input = new_input/240.0\n","\n","    if key == 'Downbeat':\n","      new_input = 1.0*new_input\n","\n","    if num == 0:\n","      song_input = new_input\n","    else:\n","      song_input = np.append(song_input, new_input, axis=1)\n","      \n","  for num, key in enumerate(outputs): # Build full outputs array\n","    if key in note_limits:\n","      new_output = df.iloc[song_idx][key][:,note_limits[key][0]:note_limits[key][1]+1][:,:]\n","\n","      if binarize_threshold is not None:\n","        new_output[new_output <= binarize_threshold] = 0\n","        new_output[new_output > binarize_threshold] = 1\n","    else:\n","      new_output = df.iloc[song_idx][key][:,np.newaxis]\n","\n","    if num == 0:\n","      song_output = new_output\n","    else:\n","      song_output = np.append(song_output, new_output, axis=1)\n","  \n","  return song_input, song_output\n","\n","\n","def get_processed_data(df, note_limits, note_counts, seq_length=200, train_ratio=0.7, validate_ratio=0.25, binarize_threshold=None):\n","\n","  num_songs = df.shape[0]\n","  train_num = int(num_songs*train_ratio)\n","  validate_num = int(num_songs*(train_ratio+validate_ratio))\n","  permutation = np.random.permutation(num_songs)\n","\n","  for song_num, song_idx in enumerate(permutation):\n","    song_input, song_output = raw_song_to_ndarray(raw_songs_df, song_idx=song_idx, note_limits=note_limits, note_counts=note_counts, binarize_threshold=binarize_threshold)\n","\n","    # Trim excess \n","    progress_at_cut = np.random.normal(loc=0.5, scale=0.15) # 'Progress' is last input variable\n","    cut_idx = np.abs(song_input[:,-1] - progress_at_cut).argmin()\n","    cut_size = song_input.shape[0]%seq_length\n","\n","    if song_input.shape[0] - cut_idx < cut_size:\n","      cut_idx = song_input.shape[0] - cut_size\n","\n","    song_input = np.delete(song_input, np.arange(cut_idx, cut_idx+cut_size), axis=0)\n","    song_output = np.delete(song_output, np.arange(cut_idx, cut_idx+cut_size), axis=0)\n","\n","    new_input_seqs = np.stack(np.split(song_input, int(song_input.shape[0]/seq_length), axis=0), axis=0)\n","    new_output_seqs = np.stack(np.split(song_output, int(song_output.shape[0]/seq_length), axis=0), axis=0)\n","\n","    if song_num == 0:\n","      x = new_input_seqs\n","      y = new_output_seqs\n","    else:\n","      x = np.append(x, new_input_seqs, axis=0)\n","      y = np.append(y, new_output_seqs, axis=0)\n","\n","    if song_num == train_num:\n","      train_idx = x.shape[0]\n","    \n","    if song_num == validate_num:\n","      validate_idx = x.shape[0]\n","  \n","  x_train = x[:train_idx,:,:]\n","  x_validate = x[train_idx:validate_idx,:,:]\n","  x_test = x[validate_idx:,:,:]\n","\n","  y_train = y[:train_idx,:,:]\n","  y_validate = y[train_idx:validate_idx,:,:]\n","  y_test = y[validate_idx:,:,:]\n","\n","  return x_train, x_validate, x_test, y_train, y_validate, y_test\n","\n","\n","def play_sequence_audio(x, y, note_limits, note_counts, volume_scaling=1.0, output_boost=1.5):\n","\n","  seq_length = x.shape[0]\n","\n","  inputs = ['Drums','Vocals','Rhythm','Lead','Tempo','Downbeat','Progress']  # 'Progress' is last input variable\n","  outputs = ['Bass']\n","  song_data = dict.fromkeys(inputs + outputs) # Dictionary with full ndarrays\n","\n","  for key in inputs:\n","    if key in note_limits:\n","      track_data = np.zeros((seq_length, 128)) # Initialize track notes\n","      slice_size = note_limits[key][1] - note_limits[key][0] + 1\n","\n","      track_data[:,note_limits[key][0]:note_limits[key][1]+1] = x[:,:slice_size]\n","      x = np.delete(x,np.s_[:slice_size],1)\n","      song_data[key] = track_data*volume_scaling\n","    else:\n","      track_data = x[:,0]\n","      x = np.delete(x,np.s_[0],1)\n","\n","      if key == 'Tempo':\n","        track_data = track_data*240.0\n","\n","      if key == 'Downbeat':\n","        track_data = track_data.astype(bool)\n","        #print(track_data)\n","      \n","      song_data[key] = track_data\n","        \n","  for key in outputs:\n","    if key in note_limits:\n","      track_data = np.zeros((seq_length, 128)) # Initialize track notes\n","      slice_size = note_limits[key][1] - note_limits[key][0] + 1\n","\n","      track_data[:,note_limits[key][0]:note_limits[key][1]+1] = y[:,:slice_size]\n","      y = np.delete(y,np.s_[:slice_size],1)\n","      song_data[key] = track_data*volume_scaling*output_boost\n","    else:\n","      track_data = y[:,0]\n","      y = np.delete(y,np.s_[0],1)\n","\n","      if key == 'Tempo':\n","        track_data = track_data*240.0\n","      \n","      if key == 'Downbeat':\n","        track_data = track_data.astype(bool)\n","        #print(track_data)\n","      \n","      song_data[key] = track_data\n","\n","  drum_track = Track(song_data['Drums'], is_drum=True, name='Drums')\n","  bass_track = Track(song_data['Bass'], program=34, is_drum=False, name='Bass')\n","  vocal_track = Track(song_data['Vocals'], program=73, is_drum=False, name='Vocals')\n","  rhythm_track = Track(song_data['Rhythm'], program=24, is_drum=False, name='Rhythm')\n","  lead_track = Track(song_data['Lead'], program=26, is_drum=False, name='Lead')\n","  tempo = song_data['Tempo']\n","  downbeat = song_data['Downbeat']\n","\n","  bass_track.transpose(-12) # Only intermittently needed for proper playback with FluidSynth Soundfont - don't use for exporting raw MIDI files\n","\n","  sequence = Multitrack(tracks=[drum_track, bass_track, vocal_track, rhythm_track, lead_track], tempo=tempo)#, downbeat=downbeat)\n","\n","  sequence.check_validity()\n","\n","  sequence.write('temp/sequence.mid')\n","  write_soundfile('temp/sequence.mid', 'temp/sequence.mp3')\n","  print('Sequence')\n","  display(Audio('temp/sequence.mp3'))\n","\n","  sequence_bass = Multitrack(tracks=[bass_track], tempo=tempo)#, downbeat=downbeat)\n","\n","  sequence_bass.write('temp/sequence_bass.mid')\n","  write_soundfile('temp/sequence_bass.mid', 'temp/sequence_bass.mp3')\n","  print('Sequence (Bass Only)')\n","  display(Audio('temp/sequence_bass.mp3'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmBfSst7YkRl","colab_type":"code","colab":{}},"source":["# Initialize dataframe\n","col_names = ['Song','Album','Drums','Bass','Vocals','Rhythm','Lead','Tempo','Downbeat','Progress']\n","raw_songs_df = pd.DataFrame(columns = col_names)\n","#raw_songs_df\n","\n","# Populate dataframe\n","note_limits, note_counts = add_songs_to_df(raw_songs_df, partial_track_match=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMbV14_5Fm9t","colab_type":"code","colab":{}},"source":["# Check df\n","\n","raw_songs_df.info()\n","\n","print('Note limits:', note_limits)\n","print('Note counts:', note_counts)\n","raw_songs_df.head()\n","\n","#manual_song_test(raw_songs_df, 'DearPrudence')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KoNHAztiGFqg","colab_type":"code","colab":{}},"source":["# Process data from df\n","\n","x_train, x_validate, x_test, y_train, y_validate, y_test = get_processed_data(df=raw_songs_df, note_limits=note_limits, note_counts=note_counts, seq_length=200, train_ratio=0.7, validate_ratio=0.25, binarize_threshold=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-LDH6ZL9mewH","colab_type":"code","colab":{}},"source":["# Listen to a sequence\n","\n","seq_num = 26\n","play_sequence_audio(x=x_train[seq_num,:,:], y=y_train[seq_num,:,:], note_limits=note_limits, note_counts=note_counts, volume_scaling=100.0)\n","\n","print(np.amax(x_train[seq_num,:,:-3]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxCHukCsuRGA","colab_type":"code","colab":{}},"source":["# Save dataset (huge)\n","\n","np.savez('temp/outfile.npz', x_train=x_train, x_validate=x_validate, x_test=x_test, y_train=y_train, y_validate=y_validate, y_test=y_test, note_limits=note_limits, note_counts=note_counts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h0I-4fGI0jhy","colab_type":"code","colab":{}},"source":["# Load dataset\n","\n","data_cache = np.load('temp/outfile.npz', allow_pickle=True)\n","\n","x_train = data_cache['x_train']\n","x_validate = data_cache['x_validate']\n","x_test = data_cache['x_test']\n","y_train = data_cache['y_train']\n","y_validate = data_cache['y_validate']\n","y_test = data_cache['y_test']\n","note_limits = data_cache['note_limits']\n","note_counts = data_cache['note_counts']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"24Vo4OjJlnBW","colab_type":"code","colab":{}},"source":["# Write all song/track names to a text file\n","\n","from contextlib import redirect_stdout\n","\n","with open('TrackList.txt', 'w') as f:\n","  with redirect_stdout(f):\n","    parse_midi_dir(list_tracks=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-Ug9SNmVB5q","colab_type":"code","colab":{}},"source":["# Basic model (for testing)\n","\n","model = tf.keras.Sequential()\n","model.add(keras.Input(shape=(200,276)))\n","model.add(layers.SimpleRNN(51, return_sequences=True))\n","\n","# Configure a model for categorical classification.\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy', # Each note is an independent 'class'\n","              metrics=['accuracy'])\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AE7EzWu9iK8V","colab_type":"code","colab":{}},"source":["# Real model\n","\n","model = tf.keras.Sequential()\n","\n","model.add(keras.Input(shape=(200,276)))\n","\n","model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n","model.add(layers.Dropout(0.2))\n","model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True)))\n","model.add(layers.Dropout(0.2))\n","model.add(layers.Dense(51, activation='sigmoid'))\n","\n","model.compile(optimizer='adam',\n","              loss='binary_crossentropy', # Each note is an independent 'class'\n","              metrics=['accuracy'])\n","\n","# Save model checkpoints\n","checkpoint = keras.callbacks.ModelCheckpoint('best.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n","callbacks_list = [checkpoint]\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JOaTM7X5mWUI","colab_type":"code","colab":{}},"source":["# Run the model\n","\n","history = model.fit(x_train, y_train, epochs=100, batch_size=32, callbacks=callbacks_list,\n","          validation_data=(x_validate, y_validate))\n","\n","model.save('bass_model.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FBi_nYdpas3f","colab_type":"code","colab":{}},"source":["# Plot loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validate'], loc='upper left')\n","plt.show()\n","\n","# Plot accuracy\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Validate'], loc='upper left')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wniAiUSCpPut","colab_type":"code","colab":{}},"source":["# Load model\n","#model = tf.keras.models.load_model('bass_model.h5')\n","model = tf.keras.models.load_model('best.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qLOro9IopcDY","colab_type":"code","colab":{}},"source":["# Predict test set\n","y_predicted = model.predict(x_test, batch_size=32)\n","\n","# Binarize\n","threshold = 0.1\n","y_predicted[y_predicted <= threshold] = 0\n","y_predicted[y_predicted > threshold] = 1\n","\n","#Sanity check\n","np.count_nonzero(y_predicted,axis=(1,2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8939kSJqp5kk","colab_type":"code","colab":{}},"source":["# Listen to a test set sequence\n","\n","seq_num = 85\n","print('ORIGINAL')\n","play_sequence_audio(x=x_test[seq_num,:,:], y=y_test[seq_num,:,:], note_limits=note_limits, note_counts=note_counts, volume_scaling=100.0)\n","\n","print('PREDICTED')\n","play_sequence_audio(x=x_test[seq_num,:,:], y=y_predicted[seq_num,:,:], note_limits=note_limits, note_counts=note_counts, volume_scaling=100.0)"],"execution_count":0,"outputs":[]}]}